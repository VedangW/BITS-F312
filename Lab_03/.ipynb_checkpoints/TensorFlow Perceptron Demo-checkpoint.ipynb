{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 03\n",
    "\n",
    "In this lab we'll try to model a perceptron in Python using the Deep Learning library TensorFlow (```tf```). ```tf``` provides us with a framework for creating models and training them.\n",
    "\n",
    "We have tried building the Perceptron model before, and we'll try to continue building it, using ```tf``` this time. By the end of this demo you'll be able to completely build a Perceptron using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the Breast Cancer dataset for training and testing the Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(test_size=0.2):\n",
    "    print (\"Using the Breast Cancer dataset.\")\n",
    "    print (\"\")\n",
    "    \n",
    "    # Download the breast cancer dataset\n",
    "    dataset = load_breast_cancer()\n",
    "    x, y = dataset['data'], dataset['target']\n",
    "    x_t, x_t_, y_t, y_t_ = train_test_split(x, y, test_size=test_size, random_state=10)\n",
    "\n",
    "    # Reshape y_train and y_test\n",
    "    y_t = np.reshape(y_t, (y_t.shape[0], 1))\n",
    "    y_t_ = np.reshape(y_t_, (y_t_.shape[0], 1))\n",
    "\n",
    "    # Print details of dataset\n",
    "    print (\"Train set:\")\n",
    "    print (\"# of samples =\", x_t.shape[0])\n",
    "    print (\"# of features =\", x_t.shape[1])\n",
    "    print (\"\")\n",
    "    print (\"Test set:\")\n",
    "    print (\"# of samples =\", x_t_.shape[0])\n",
    "    print (\"# of features =\", x_t_.shape[1])\n",
    "    \n",
    "    return x_t, y_t, x_t_, y_t_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the Breast Cancer dataset.\n",
      "\n",
      "Train set:\n",
      "# of samples = 455\n",
      "# of features = 30\n",
      "\n",
      "Test set:\n",
      "# of samples = 114\n",
      "# of features = 30\n"
     ]
    }
   ],
   "source": [
    "x_t, y_t, x_t_, y_t_ = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the Perceptron**\n",
    "\n",
    "We can build a model in TensorFlow from the ground up, by defining the layers and training the model ourselves or by using the predefined functionality in Keras (which is available in TensorFlow as ```tf.keras```). \n",
    "\n",
    "The ground-up way is used when we need more control of the model or want to introduce custom layers or operations. The predefined functionality is used when we either want to make a standard model or make a model and compile it quickly.\n",
    "\n",
    "We start with the ground-up way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "\n",
    "ALPHA = 0.01 # Learning rate\n",
    "NUM_EPOCHS = 10 # Number of epochs\n",
    "NUM_FEATURES = x_t.shape[1] # Number of features for that perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building the model\n",
    "\n",
    "# Placeholders for the features and labels\n",
    "x_train = tf.placeholder(tf.float32, shape=x_t.shape)\n",
    "y_train = tf.placeholder(tf.float32, shape=(y_t.shape[0], 1))\n",
    "\n",
    "# Weight and bias variables\n",
    "W = tf.Variable(tf.zeros([NUM_FEATURES, 1]), tf.float32)\n",
    "b = tf.Variable(tf.zeros([1, 1]), tf.float32)\n",
    "\n",
    "# Calculate the predicted value\n",
    "y_hat = tf.sigmoid(tf.add(tf.matmul(x_train, W), b))\n",
    "\n",
    "# Calculate errors\n",
    "errors = y_train - y_hat\n",
    "\n",
    "# Calculate the gradients for weights and bias\n",
    "del_W = tf.matmul(tf.transpose(x_train), errors)\n",
    "del_b = tf.reduce_sum(errors, 0)\n",
    "\n",
    "# Update weights\n",
    "W_ = W + ALPHA*del_W\n",
    "b_ = b + ALPHA*del_b\n",
    "\n",
    "# Create an op and assign operations to it\n",
    "step = tf.group(W.assign(W_), b.assign(b_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #01: train_loss = 0.11978\n",
      "Epoch #02: train_loss = 0.61978\n",
      "Epoch #03: train_loss = -0.38022\n",
      "Epoch #04: train_loss = 0.61978\n",
      "Epoch #05: train_loss = -0.38022\n",
      "Epoch #06: train_loss = 0.61978\n",
      "Epoch #07: train_loss = 0.61978\n",
      "Epoch #08: train_loss = -0.38022\n",
      "Epoch #09: train_loss = 0.61978\n",
      "Epoch #10: train_loss = -0.38022\n"
     ]
    }
   ],
   "source": [
    "## Session\n",
    "\n",
    "# Create a TensorFlow session\n",
    "with tf.Session() as sess:\n",
    "    # Initialize global variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Train the model\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        print (\"Epoch #{0:02d}\".format(i+1) + \":\", end=' ')\n",
    "        _, outs = sess.run([step, errors], feed_dict={x_train:x_t, y_train:y_t})\n",
    "        outs = [item for sublist in outs for item in sublist]\n",
    "        training_error = np.mean(outs)\n",
    "        print (\"train_loss = {0:0.5f}\".format(training_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Predefined Functionality**\n",
    "\n",
    "Now we train using the predefined functionality. We use ```tf.keras``` for this. It is much easier to build the model this way, as can be seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 0s 283us/step - loss: 0.6198 - acc: 0.3802\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 74us/step - loss: 0.6198 - acc: 0.3802\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.6198 - acc: 0.3802\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 83us/step - loss: 0.6198 - acc: 0.3802\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.6198 - acc: 0.3802\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 73us/step - loss: 0.6198 - acc: 0.3802\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 52us/step - loss: 0.6198 - acc: 0.3802\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 75us/step - loss: 0.6198 - acc: 0.3802\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.6198 - acc: 0.3802\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 93us/step - loss: 0.6198 - acc: 0.3802\n",
      "\n",
      "Evaluating...\n",
      "114/114 [==============================] - 0s 689us/step\n",
      "Test loss = 0.6578947410248873\n",
      "Test Accuracy = 0.3421052636807425\n"
     ]
    }
   ],
   "source": [
    "def perceptron_loss(y_true, y_pred):\n",
    "    \"\"\" Loss function of perceptron is\n",
    "        just the difference between true \n",
    "        and predicted values. \"\"\"\n",
    "    return y_true - y_pred\n",
    "\n",
    "# Create a sequential model\n",
    "perceptron = tf.keras.models.Sequential()\n",
    "\n",
    "# Add a neuron to the model\n",
    "perceptron.add(tf.keras.layers.Dense(1, activation=tf.sigmoid))\n",
    "\n",
    "# Compile model\n",
    "perceptron.compile(optimizer='adam',\n",
    "              loss=perceptron_loss,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print (\"Training model...\")\n",
    "print (\"\")\n",
    "perceptron.fit(x_t, y_t, epochs=10)\n",
    "print (\"\")\n",
    "\n",
    "print (\"Evaluating...\")\n",
    "test_loss, test_acc = perceptron.evaluate(x_t_, y_t_)\n",
    "print (\"Test loss =\", test_loss)\n",
    "print (\"Test Accuracy =\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the summary of the model, i.e., number and names of layers, number of parameters for each layer, type of each layer, total number of trainable parameters and more using ```model.summary()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  31        \n",
      "=================================================================\n",
      "Total params: 31\n",
      "Trainable params: 31\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display the summary of the model\n",
    "perceptron.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Object Oriented Way**\n",
    "\n",
    "Following is an Object-Oriented way of building the Perceptron. There is a ```Perceptron``` class which of course models the way the Perceptron is built. The method, ```_forward_propagate()``` builds the graph for the forward propagation computations while the ```_back_propagate()``` method builds the graph for the backpropagation computations. It is very important to note that neither of these methods actually does the computations, but only build the corresponding graphs. The training is managed in the ```fit()``` method which contains the session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, num_features, alpha=0.01):\n",
    "        \"\"\" Class to model a Perceptron in TensorFlow.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            num_features: int\n",
    "                Number of features it will take as input\n",
    "            alpha: float\n",
    "                Learning rate of perceptron.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = alpha\n",
    "        \n",
    "        self.W = tf.Variable(tf.zeros([self.num_features, 1]), tf.float32)\n",
    "        self.b = tf.Variable(tf.zeros([1, 1]), tf.float32)\n",
    "        \n",
    "    def _forward_propagate(self):\n",
    "        \"\"\" Forward propagation of Perceptron. \"\"\"\n",
    "        \n",
    "        # Predicted value by forward propagation\n",
    "        self.x_train = tf.cast(self.x_train, tf.float32)\n",
    "        self.y_hat = tf.sigmoid(tf.add(tf.matmul(self.x_train, self.W), \n",
    "                                       self.b))\n",
    "        \n",
    "    def _back_propagate(self):\n",
    "        \"\"\" Backpropagation of Perceptron. \"\"\"\n",
    "        \n",
    "        # Calculate error\n",
    "        self.error = self.y_train - self.y_hat\n",
    "\n",
    "        # Calculate the gradients for the weight and the bias\n",
    "        self.del_W = tf.matmul(tf.transpose(self.x_train), self.error)\n",
    "        self.del_b = tf.reduce_sum(self.error, 0)\n",
    "        \n",
    "        # Calculate updated values of the weight and the bias\n",
    "        self.W_ = self.W + self.learning_rate * self.del_W\n",
    "        self.b_ = self.b + self.learning_rate * self.del_b\n",
    "\n",
    "        # Assign updated values and create an op\n",
    "        self.training_op = tf.group([self.W.assign(self.W_),\n",
    "                                     self.b.assign(self.b_)])\n",
    "\n",
    "    def fit(self, x_t, y_t, epochs):\n",
    "        \"\"\" Training the perceptron.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            x_t: np.ndarray\n",
    "                The training features.\n",
    "            y_t: np.ndarray\n",
    "                The training labels.\n",
    "            epochs: int\n",
    "                Number of epochs to train for.\n",
    "        \"\"\"\n",
    "\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # Placeholders for training features and labels\n",
    "        self.x_train = tf.placeholder(tf.float32, shape=x_t.shape)\n",
    "        self.y_train = tf.placeholder(tf.float32, shape=(y_t.shape[0], 1))\n",
    "        \n",
    "        # Perform forward and backward propagation\n",
    "        self._forward_propagate()\n",
    "        self._back_propagate()\n",
    "        \n",
    "        # Train the model\n",
    "        with tf.Session() as sess:\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "\n",
    "            for i in range(self.epochs):\n",
    "                print (\"Epoch #{0:02d}\".format(i+1) + \":\", end=' ')\n",
    "                _, outs = sess.run([self.training_op, self.error], feed_dict={self.x_train:x_t,\n",
    "                                                                         self.y_train:y_t})\n",
    "                outs = [item for sublist in outs for item in sublist]\n",
    "                training_error = np.mean(outs)\n",
    "                print (\"train_loss = {0:0.5f}\".format(training_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #01: train_loss = 0.11978\n",
      "Epoch #02: train_loss = 0.61978\n",
      "Epoch #03: train_loss = -0.38022\n",
      "Epoch #04: train_loss = 0.61978\n",
      "Epoch #05: train_loss = -0.38022\n",
      "Epoch #06: train_loss = 0.61978\n",
      "Epoch #07: train_loss = 0.61978\n",
      "Epoch #08: train_loss = -0.38022\n",
      "Epoch #09: train_loss = 0.61978\n",
      "Epoch #10: train_loss = -0.38022\n"
     ]
    }
   ],
   "source": [
    "# Train the perceptron\n",
    "perceptron = Perceptron(num_features=NUM_FEATURES)\n",
    "perceptron.fit(x_t.astype(np.float32), y_t.astype(np.float32), epochs=NUM_EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

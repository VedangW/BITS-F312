{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Triple\" Perceptron\n",
    "\n",
    "There's no such thing as a \"Triple\" perceptron obviously. But for the sake of this question let's say there is. The idea is that there will be 3 perceptrons our model. They perform predictions as follows:\n",
    "\n",
    "All perceptrons have an input dimension of 3. All perceptrons take the same inputs, $x_1$, $x_2$ and $x_3$ in one iteration for one sample. The activation function that each perceptron uses is ```ReLU(.)```.\n",
    "\n",
    "Let $\\hat{y_1}$, $\\hat{y_2}$ and $\\hat{y_3}$ be the predictions made after the forward propagation of each perceptron. Your final prediction will be $\\tau(\\hat{y_1} + \\hat{y_2} - \\hat{y_3})$ where $\\tau(.)$ is a custom activation function that we will make such that:\n",
    "\n",
    "\n",
    "\n",
    "Weights are updated in the usual way, i.e., by Gradient Descent as in the demo.\n",
    "\n",
    "We will use the Iris dataset for checking how well this model performs. The metric used for this will be accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 4), (30, 4), (120,), (30,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Iris dataset\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris['data'], iris['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "num_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.7, 3. , 5.2, 2.3],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [5.1, 3.5, 1.4, 0.2],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [5.9, 3. , 5.1, 1.8],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [4.9, 3. , 1.4, 0.2]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building the model\n",
    "\n",
    "def tau(y_hat_1, y_hat_2, y_hat_3):\n",
    "    add_op = tf.add(y_hat_1, y_hat_2)\n",
    "    sub_op = tf.sub(temp, y_hat_3)\n",
    "    \n",
    "    if sub_op > thres_1:\n",
    "        return tf.constant(2)\n",
    "    elif sub_op > thres_2 and sub_op < thres_1:\n",
    "        return tf.constant(1)\n",
    "    \n",
    "    return tf.constant(0)\n",
    "    \n",
    "# Placeholders for the features and labels\n",
    "x_train = tf.placeholder(tf.float32, shape=X_train.shape)\n",
    "y_train = tf.placeholder(tf.float32, shape=(y_train.shape[0], 1))\n",
    "\n",
    "# Weight and bias variables\n",
    "W_1 = tf.Variable(tf.zeros([NUM_FEATURES, 1]), tf.float32)\n",
    "b_1 = tf.Variable(tf.zeros([1, 1]), tf.float32)\n",
    "\n",
    "W_2 = tf.Variable(tf.zeros([NUM_FEATURES, 1]), tf.float32)\n",
    "b_2 = tf.Variable(tf.zeros([1, 1]), tf.float32)\n",
    "\n",
    "W_3 = tf.Variable(tf.zeros([NUM_FEATURES, 1]), tf.float32)\n",
    "b_3 = tf.Variable(tf.zeros([1, 1]), tf.float32)\n",
    "\n",
    "y_hat_1 = tf.nn.relu(tf.add(tf.matmul(x_train, W_1), b_1))\n",
    "y_hat_2 = tf.nn.relu(tf.add(tf.matmul(x_train, W_2), b_2))\n",
    "y_hat_3 = tf.nn.relu(tf.add(tf.matmul(x_train, W_3), b_3))\n",
    "\n",
    "y_hat = tau(y_hat_1, y_hat_2, y_hat_3)\n",
    "\n",
    "# Calculate errors\n",
    "errors = y_train - y_hat\n",
    "\n",
    "# Calculate the gradients for weights and bias\n",
    "del_W = tf.matmul(tf.transpose(x_train), errors)\n",
    "del_b = tf.reduce_sum(errors, 0)\n",
    "\n",
    "# Update weights\n",
    "W_1_ = W_1 + ALPHA*del_W\n",
    "b_1_ = b_1 + ALPHA*del_b\n",
    "\n",
    "W_2_ = W_2 + ALPHA*del_W\n",
    "b_2_ = b_2 + ALPHA*del_b\n",
    "\n",
    "W_3_ = W_3 + ALPHA*del_W\n",
    "b_3_ = b_3 + ALPHA*del_b\n",
    "\n",
    "# Create assign ops\n",
    "step_1 = tf.group(W_1.assign(W_1_), b_1.assign(b_1_))\n",
    "step_2 = tf.group(W_2.assign(W_2_), b_2.assign(b_2_))\n",
    "step_3 = tf.group(W_3.assign(W_3_), b_3.assign(b_3_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

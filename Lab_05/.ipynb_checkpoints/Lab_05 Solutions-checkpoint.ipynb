{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification on Devanagari Characters\n",
    "\n",
    "Devanagari is the script in which Sanskrit, Hindi and Marathi (at least) are written. Optical Character Recognition algorithms aim to train a model to recognize which character of a language is written in an image (we'll discuss this in the next lab). For instance, consider this image:\n",
    "\n",
    "<img src=\"q_data/g.jpg\" alt=\"The character G\" width=\"200\"/>\n",
    "\n",
    "The model should predict that the handwritten English character in this image is G. A similar OCR can be developed for Devanagari characters (this was the course project in the Machine Learning course in 2017-18, Sem 1).\n",
    "\n",
    "**BUT**\n",
    "\n",
    "We're not going to implement this right now. It's not possible to even train such a model in 2 hours. Here is what you have to do instead:\n",
    "\n",
    "Implement a binary classifier which predicts if the given image contains a Devanagari character or not. Note that numbers and purna viraam (full stops) are not characters and everything else is.\n",
    "\n",
    "***\n",
    "**Dataset Description**\n",
    "\n",
    "There is a directory named ```q_data``` in the same directory as this notebook which contains two directories, ```chars``` (containing images of characters) and ```non_chars``` (containing images of non-characters).\n",
    "\n",
    "This dataset is part of the dataset developed by the TAs and students in the Machine Learning course (BITS F464) in 2017-18, Sem 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility functions\n",
    "\n",
    "def show_image(img):\n",
    "    \"\"\" Call this to plot the image. \"\"\"\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the dataset only has 101 images in total. If only there was some way we could have more such data. Hmmm...\n",
    "\n",
    "So you've noticed you need data augmentation for this application. But what kind? Do you really want to tell your model that the mirror image of a Devanagari 'ya' for example is also a Devanagari character? \n",
    "\n",
    "We can apply several other ways of data augmentation here. We'll go with up-shifting and right-shifting. \n",
    "\n",
    "The following two functions up-shift and down-shift images by ```pixel``` number of pixels. Hint: Treat images as numpy arrays for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_up(img, pixels=3):\n",
    "    \"\"\" Shift image, 'img' up by 'pixel'\n",
    "        number of pixels.\n",
    "    \"\"\"\n",
    "    width = img.shape[0]\n",
    "    height = img.shape[1]\n",
    "    \n",
    "    for j in range(width):\n",
    "        for i in range(height):\n",
    "            if (j < width - pixels and j > pixels):\n",
    "                img[j][i] = img[j+pixels][i]\n",
    "            else:\n",
    "                img[j][i] = 255\n",
    "        \n",
    "    return img\n",
    "\n",
    "def translate_down(img, pixels=3):\n",
    "    \"\"\" Shift image, 'img' down by 'pixel'\n",
    "        number of pixels.\n",
    "    \"\"\"\n",
    "    width = img.shape[0]\n",
    "    height = img.shape[1]\n",
    "    \n",
    "    for j in range(width, 1, -1):\n",
    "        for i in range(height):\n",
    "            if (j < width and j > pixels):\n",
    "                img[j][i] = img[j-pixels][i]\n",
    "                \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load and preprocess the image at the same time.\n",
    "\n",
    "Here's what the preprocessing will involve:\n",
    "1. The sizes of different images are different. The CNN we'll make won't like this. We'll need to fix it. (See [Resizing image](https://medium.com/@manivannan_data/resize-image-using-opencv-python-d2cdbbc480f0))\n",
    "2. The images are kind of grainy (minute white dots in the image). We need to make them into a single continuous blob. (See [Morphological Transforms](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html), [Denoising](https://docs.opencv.org/3.0-beta/modules/photo/doc/denoising.html))\n",
    "3. Do we really need such rich color gradients in this image? Can we just make the foreground into one solid color and the background into another? (See [Adaptive Thresholding and Otsu's Binarization](https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(img_path, translate=None):\n",
    "    \"\"\" Preprocess an image to give as an input to the\n",
    "        ConvNet. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        img_path: str\n",
    "            Path to image\n",
    "        translate: None, 'up' or 'down'\n",
    "            If to translate the image and how to translate.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        img: cv2 image\n",
    "            The original image read.\n",
    "        final: cv2 image\n",
    "            The transformed image.\n",
    "    \"\"\"\n",
    "    # Initialize kernel for morphological transforms\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "\n",
    "    # Read the image as grayscale\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    resized = cv2.resize(img, (218, 192), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Denoising by blurring\n",
    "    denoised = cv2.fastNlMeansDenoising(resized, 60, 7, 21)\n",
    "\n",
    "    # Applying Otsu's binarization\n",
    "    ret, thresh = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "    # Morphological transforms - opening and closing the picture\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Resize to a smaller size\n",
    "    final = cv2.resize(closing, (130, 118), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "    if translate is not None:\n",
    "        if translate == 'up':\n",
    "            final = translate_up(final)\n",
    "        elif translate == 'down':\n",
    "            final = translate_down(final)\n",
    "        \n",
    "    return img, final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your handiwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, final = transform_image('non_chars/530.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\" Function to load data. \"\"\"\n",
    "    \n",
    "    chars = os.listdir('chars')\n",
    "    non_chars = os.listdir('non_chars')\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    # Load char images\n",
    "    char_images = []\n",
    "    for i in tqdm(range(len(chars)), desc='Preprocessing chars...'):\n",
    "        fname = chars[i]\n",
    "        \n",
    "        _, final = transform_image('chars/' + fname)\n",
    "        _, final_up = transform_image('chars/' + fname, 'up')\n",
    "        _, final_down = transform_image('chars/' + fname, 'down')\n",
    "        \n",
    "        final = np.reshape(final, (final.shape[0], final.shape[1], 1))\n",
    "        final_up = np.reshape(final_up, (final_up.shape[0], final_up.shape[1], 1))\n",
    "        final_down = np.reshape(final_down, (final_down.shape[0], final_down.shape[1], 1))\n",
    "        \n",
    "        char_images.append(final)\n",
    "        char_images.append(final_up)\n",
    "        char_images.append(final_down)\n",
    "        \n",
    "        for i in range(3):\n",
    "            labels.append(1)\n",
    "        \n",
    "    # Load non-char images\n",
    "    non_char_images = []\n",
    "    for i in tqdm(range(len(non_chars)), desc='Preprocessing non-chars...'):\n",
    "        fname = non_chars[i]\n",
    "\n",
    "        _, final = transform_image('non_chars/' + fname)\n",
    "        _, final_up = transform_image('non_chars/' + fname, 'up')\n",
    "        _, final_down = transform_image('non_chars/' + fname, 'down')\n",
    "        \n",
    "        final = np.reshape(final, (final.shape[0], final.shape[1], 1))\n",
    "        final_up = np.reshape(final_up, (final_up.shape[0], final_up.shape[1], 1))\n",
    "        final_down = np.reshape(final_down, (final_down.shape[0], final_down.shape[1], 1))\n",
    "        \n",
    "        non_char_images.append(final)\n",
    "        non_char_images.append(final_up)\n",
    "        non_char_images.append(final_down)\n",
    "        \n",
    "        for i in range(3):\n",
    "            labels.append(0)\n",
    "        \n",
    "    # Random Shuffling\n",
    "    X = np.array(char_images + non_char_images)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    s = np.arange(X.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    \n",
    "    X = X[s]\n",
    "    y = y[s]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing chars...: 100%|██████████| 51/51 [00:44<00:00,  1.22it/s]\n",
      "Preprocessing non-chars...: 100%|██████████| 50/50 [00:40<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((303, 118, 130, 1), (303, 2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Building**\n",
    "\n",
    "Build a ConvNet to make predictions on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "POOL_SIZE = (2, 2) # These are usually the values\n",
    "KERNEL_SIZE = (3, 3)\n",
    "\n",
    "# Create model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, input_shape=(118, 130, 1), \n",
    "                 activation='relu', kernel_size=KERNEL_SIZE))\n",
    "model.add(Conv2D(32, activation='relu', kernel_size=KERNEL_SIZE))\n",
    "model.add(MaxPooling2D(pool_size=POOL_SIZE))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vedang/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 242 samples, validate on 61 samples\n",
      "Epoch 1/1\n",
      "242/242 [==============================] - 20s 83ms/step - loss: 8.0675 - acc: 0.4835 - val_loss: 7.9269 - val_acc: 0.5082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f607e9e4668>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=20, epochs=1, validation_split=0.2, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

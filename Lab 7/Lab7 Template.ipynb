{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT WORD PREDICTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train a RNN which predicts the next word given a sequence of words. We take some work of Shakespeare for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', \n",
    "                                       'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the size is too big, we'll take a subset of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning \n",
    "\n",
    "Our preprocessing is going to involve using a `Tokenizer` to convert the text from sequence of words (strings) into sequence of integers, after removing punctuation and converting to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True)\n",
    "tokenizer.fit_on_texts([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"first citizen before we proceed any further hear me speak all speak speak first citizen you are all resolved rather to die than to famish all resolved resolved first citizen first you know caius marcius is chief enemy to the people all we know't we know't first citizen let us kill him and we'll have corn at our own price is't a verdict all no more talking on't let it be done away away second citizen one word good citizens first citizen we are accounted poor citizens the patricians good what authority surfeits on would relieve us if they would yield us but the superfluity while it were wholesome we might guess they relieved us humanely but they think we are too dear the leanness that afflicts us the object of our misery is as an inventory to particularise their abundance our sufferance is a gain to them let us revenge this with our pikes ere we become rakes for the gods know i speak this in hunger for bread not in thirst for revenge second citizen would you proceed especially against ca\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "# Printing first 1000 characters of text\n",
    "' '.join(tokenizer.index_word[i] for i in sequence)[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Labels\n",
    "\n",
    "To train the model, we use a certain number of words which we will feed into the network as features with the next word as the label. For example, if we set `training_length = 50`, then the model will take in 50 words as features and the 51st word as the label. \n",
    "\n",
    "We can make multiple training examples by slicing at different points. We can use the first 50 words as features with the 51st as a label, then the 2nd through 51st word as features and the 52nd as the label, then 3rd - 52nd with 53rd as label and so on. This gives us much more data to train on and the performance of the model is proportional to the amount of training data.\n",
    "\n",
    "Complete the function to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_LENGTH = 50\n",
    "\n",
    "def make_sequences(text, training_length=50):\n",
    "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
    "\n",
    "    # Create the tokenizer object and train on texts\n",
    "    tokenizer = \n",
    "\n",
    "    # Creating look-up dictionaries and reverse look-ups\n",
    "    word_idx = tokenizer.word_index\n",
    "    idx_word = tokenizer.index_word\n",
    "    num_words = len(word_idx) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "\n",
    "    print(f'There are {num_words} unique words.')\n",
    "\n",
    "    # Convert text to sequence of integers\n",
    "    sequence = \n",
    "\n",
    "    training_seq = []\n",
    "    labels = []\n",
    "\n",
    "    # Create multiple training examples from each sequence\n",
    "    \n",
    "\n",
    "    print(f'There are {len(training_seq)} training sequences.')\n",
    "\n",
    "    # Return everything needed for setting up the model\n",
    "    return word_idx, idx_word, num_words, word_counts, training_seq, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3105 unique words.\n",
      "There are 17975 training sequences.\n"
     ]
    }
   ],
   "source": [
    "word_idx, idx_word, num_words, word_counts, features, labels = make_sequences(text, TRAINING_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is now represented as a sequence of integers. Let's look at an example of a few features and the corresponding labels. The label is the next word in the sequence after the first 50 words. We'll see the 3rd generated sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 358, 184, 163, 94, 30, 81, 35, 81, 81]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 10 words of 3rd sequence\n",
    "features[3][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete this function which prints the feature and its corresponding label in words given a sequence number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_answer(index):\n",
    "    \"\"\"Find label corresponding to features for index in training data\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: we proceed any further hear me speak all speak speak first citizen you are all resolved rather to die than to famish all resolved resolved first citizen first you know caius marcius is chief enemy to the people all we know't we know't first citizen let us kill him and\n",
      "\n",
      "Label:  we'll\n"
     ]
    }
   ],
   "source": [
    "find_answer(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "Next we need to take the features and labels and convert them into training and validation data. The following function does this by splitting the data - after random shuffling because the features were made in sequential order - based on the `train_fraction` specified. All the inputs are converted into numpy arrays which is the correct input to a keras neural network. One important step is to convert the labels to one hot encoded vectors because our network will be trained using `categorical_crossentropy` and makes a prediction for each word in the vocabulary.\n",
    "\n",
    "Complete the function below to achieve the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "TRAIN_FRACTION = 0.8\n",
    "\n",
    "def create_train_valid(features,\n",
    "                       labels,\n",
    "                       num_words,\n",
    "                       train_fraction=TRAIN_FRACTION):\n",
    "    \"\"\"Create training and validation features and labels after shuffling generated set.\"\"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14380, 50)\n",
      "(14380, 3105)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = create_train_valid(\n",
    "    features, labels, num_words)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "With data encoded as integers, we're ready to build the recurrent neural network. This model is relatively simple and uses an LSTM cell as the heart of the network. After converting the words into embeddings, we pass them through a single LSTM layer, then into a fully connected layer with `relu` activation before the final output layer with a `softmax` activation. The final layer produces a probability for every word in the vocab. \n",
    "\n",
    "When training, these predictions are compared to the actual label using the `categorical_crossentropy` to calculate a loss. The parameters (weights) in the network are then updated using the Adam optimizer (a variant on Stochastic Gradient Descent) with gradients calculated through backpropagation.\n",
    "\n",
    "We'll use shape of embeddings as 100. Complete the below function which makes the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 100)         310500    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3105)              400545    \n",
      "=================================================================\n",
      "Total params: 761,605\n",
      "Trainable params: 761,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SHAPE = 100\n",
    "LSTM_CELLS = 64\n",
    "\n",
    "def make_word_level_model(num_words,\n",
    "                          embedding_shape,\n",
    "                          lstm_cells=64):\n",
    "    \"\"\"Make a word level recurrent neural network.\"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Map words to an embedding\n",
    "    \n",
    "    # Add LSTM layer\n",
    "    \n",
    "    # Output layer\n",
    "    \n",
    "    # Compile the model\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = make_word_level_model(\n",
    "    num_words,\n",
    "    embedding_shape=EMBEDDING_SHAPE,\n",
    "    lstm_cells=LSTM_CELLS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model needs a loss to minimize (`categorical_crossentropy`) as well as a method for updating the weights using the gradients (`Adam`). We will also monitor accuracy which is not a good loss but can give us a more interpretable measure of the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "We can now train the model on our training examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14380 samples, validate on 3595 samples\n",
      "Epoch 1/5\n",
      "14380/14380 [==============================] - 44s 3ms/step - loss: 6.6530 - acc: 0.0363 - val_loss: 6.3939 - val_acc: 0.0403\n",
      "Epoch 2/5\n",
      "14380/14380 [==============================] - 41s 3ms/step - loss: 6.2989 - acc: 0.0416 - val_loss: 6.4285 - val_acc: 0.0426\n",
      "Epoch 3/5\n",
      "14380/14380 [==============================] - 41s 3ms/step - loss: 6.1900 - acc: 0.0430 - val_loss: 6.5281 - val_acc: 0.0426\n",
      "Epoch 4/5\n",
      "14380/14380 [==============================] - 42s 3ms/step - loss: 6.1105 - acc: 0.0430 - val_loss: 6.5503 - val_acc: 0.0423\n",
      "Epoch 5/5\n",
      "14380/14380 [==============================] - 42s 3ms/step - loss: 6.0265 - acc: 0.0437 - val_loss: 6.6388 - val_acc: 0.0456\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy - both training and validation - increase over time and the loss decreases over time which gives us indication that our model is getting better with training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Output\n",
    "\n",
    "Now for the fun part: we get to use our model to generate new Shakespearean verse. To do this, we feed the network a seed sequence, have it make a prediction, add the predicted word to the sequence, and make another prediction for the next word. We continue this for the number of words that we want. We compare the generated output to the actual abstract to see if we can tell the difference!\n",
    "\n",
    "Complete the below function to achieve the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def generate_output(model,\n",
    "                    sequence,\n",
    "                    new_words=50,\n",
    "                    training_length = TRAINING_LENGTH):\n",
    "    \"\"\"Generate `new_words` words of output from a trained model.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sequence :\n",
      " how lies their battle know you on which side they have placed their men of trust cominius as i guess marcius their bands i' the vaward are the antiates of their best trust o'er them aufidius their very heart of hope marcius i do beseech you by all the battles\n",
      "Generated Sequence :\n",
      " < --- > the people and the people and the people and the\n",
      "Actual Sequence :\n",
      " < --- > wherein we have fought by the blood we have shed\n"
     ]
    }
   ],
   "source": [
    "generate_output(model, sequence, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
